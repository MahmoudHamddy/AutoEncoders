{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c42nqZI0tmb"
      },
      "source": [
        "---\n",
        "# Cairo University Faculty of Engineering\n",
        "## Deep Learning\n",
        "## Assignment 3\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0gWpg1K0tme"
      },
      "source": [
        "Please write your full name here\n",
        "- **Name** : \"-----------\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdQLJE6OnCGi"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zvUMjkBmwzz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNd1fty3m2Kd"
      },
      "source": [
        "# Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oFSIjGVxPud"
      },
      "source": [
        "## 1.1 Autoencoders Latent Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6NxYiQI0qiw"
      },
      "source": [
        "The beauty of autoencoders is the possibility to see these internal representations. The bottleneck layer enforces data compression by having fewer units than input and output layers. Further limiting this layer to two or three units enables us to see how the autoencoder is organizing the data internally in two or three-dimensional latent space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzW-OkOI1mWp"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dXkJQrb1v0q"
      },
      "source": [
        "We will use the MNIST numbers dataset but load it using a different method\n",
        "- Use tfds (tensorflow_datasets) to load the mnist numbers dataset\n",
        "- In the tfds.load function:\n",
        "  - Split the data into test and train\n",
        "  - Shuffle the data\n",
        "  - Make sure to load the labels with the data ('as_supervised' flag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKkydr_52sSz"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "(ds_train, ds_test_) = ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuLCsrhj5wf7"
      },
      "source": [
        "**For ds_train**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7bkGpQW23Ju"
      },
      "source": [
        "- Fill in the missing code in 'preprocess function' to:\n",
        "  - Cast the image to tf.float32\n",
        "  - Normalize the image by dividing it by 255.0\n",
        "- Batch the dataset\n",
        "- Apply preprocess function to ds_train using 'map'\n",
        "- cache and prefetch ds_train  (use tf.data.AUTOTONE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**\n",
        "\n",
        "What is caching and prefetching? And why should we use them?"
      ],
      "metadata": {
        "id": "oX1UP-7R0z7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER"
      ],
      "metadata": {
        "id": "23ge3Mjq089g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP1xYYesxVlD"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "\n",
        "def preprocess(image, label):\n",
        "    # cast image to tf.float32\n",
        "    image =\n",
        "    # normalize image\n",
        "    image =\n",
        "\n",
        "    return image, image\n",
        "\n",
        "# batch dataset\n",
        "ds_train =\n",
        "# apply preprocess using map\n",
        "ds_train =\n",
        "# cache data\n",
        "ds_train =\n",
        "# prefetch data\n",
        "ds_train ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0-dFyfr52Oz"
      },
      "source": [
        "**For ds_test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt8P8tL958FY"
      },
      "source": [
        "- Apply the same previous transformations to ds_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GraqHpXu53aZ"
      },
      "outputs": [],
      "source": [
        "## Your Code here\n",
        "# batch dataset\n",
        "ds_test =\n",
        "# apply preprocess using map\n",
        "ds_test =\n",
        "# cache data\n",
        "ds_test =\n",
        "# prefetch data\n",
        "ds_test ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZmrzhBo6PPW"
      },
      "source": [
        "- Fill the missing code for the following function to return image and label for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nORHdnPJ6UQX"
      },
      "outputs": [],
      "source": [
        "# return label for testing\n",
        "def preprocess_with_label(image, label):\n",
        "  #cast image to tf.float32\n",
        "  image =\n",
        "  # Normalize image by dividing by 255.0\n",
        "  image =\n",
        "  # return image and label\n",
        "  return\n",
        "\n",
        "# Use map to apply the above function to ds_test_ then batch (size=500), cache and prefectch it\n",
        "ds_test_label ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0GXQkfq7UWY"
      },
      "source": [
        "### Latent Space Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csNmVORa7YK6"
      },
      "source": [
        "The plotting functions help visualize the encoding of inputs from high dimension into 2D latent space, and decoding back to the original dimension. The functions produces two plots:\n",
        "\n",
        "- **Encoder map** shows the mapping from input images to coordinates (z1,z2) in latent space, with digit labels displayed in colorbar\n",
        "\n",
        "- **Decoder** **grid** shows reconstructions from a grid of latent space coordinates (z1,z2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCRq61U_8zNn"
      },
      "source": [
        "![Latent space visualization](https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/latent_space_plots_noaxis.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doa7VkPm9OCv"
      },
      "source": [
        "Build **TWO** fully connected models:\n",
        "1. Encoder: (1 layer only)\n",
        "  - layer 0 reshape input image to a vector\n",
        "  - layer 1: 64 neurons\n",
        "  - layer 2: 32 neurons\n",
        "  - layer 3: 16 neurons\n",
        "  - layer 4/encoding_dim : 2 neurons (no activation)\n",
        "  - layer 5: Batch Normalization [What?](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)\n",
        "2. Decoder: (1 layer only)\n",
        "  - layer 0 : 16 neurons\n",
        "  - layer 1 : 32 neurons\n",
        "  - layer 2 : 64 neurons\n",
        "  - layer 3 : output image vector shape (sigmoid)\n",
        "  - layer 4: reshape vector to a 2D image\n",
        "\n",
        "- All layers use ReLu activation unless stated otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**\n",
        "\n",
        "What is batch normalization? Why should  we use it?"
      ],
      "metadata": {
        "id": "jBhn-UKn13Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER"
      ],
      "metadata": {
        "id": "CaFfrC3v2AF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FErjbvMx8Gwj"
      },
      "outputs": [],
      "source": [
        "encoder =\n",
        "\n",
        "decoder =\n",
        "\n",
        "autoencoder = tf.keras.Sequential([encoder, decoder])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbDvzQOI-vlK"
      },
      "source": [
        "Compile the model using adam optimizer and binary cross entropy error loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtwnJVmY-vlK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hrlHYTB-UWU"
      },
      "source": [
        "- Train the network for `epochs=60` epochs and `validation_data=ds_test`, and visualize a few reconstructed samples.\n",
        "    - Load the weights in ./checkpoints/autoencoder to your model as initial weights BEFORE training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFGQ_pDN-kTJ"
      },
      "outputs": [],
      "source": [
        "#### Your Code Here\n",
        "# load the weights in ./checkpoints/autoencoder to ur built model\n",
        "\n",
        "# train model\n",
        "history="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-2e5mvJBPcn"
      },
      "source": [
        "- Load a batch of images from test set (ds_test) and display 10 of them in a SINGLE figure:\n",
        "  - Display the original images in 1 row\n",
        "  - Display their reconstruction from autoencoder in 2nd row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zwnO8FfBcjY"
      },
      "outputs": [],
      "source": [
        "## Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooiM1FH7QgPX"
      },
      "source": [
        "We can use autoencoder to generate images!! Now that we have a trained autoencoder, we can ignore the encoder and use only the\n",
        "decoder to sample from the latent variables to generate images. As we\n",
        "did not use any activation in the last layer before the latent variables, the latent space is\n",
        "unbounded and can be any real floating numbers, and there are hundreds of them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AenflgAZPHJ1"
      },
      "source": [
        "Let's look at the latent space.\n",
        "- Load a batch of images from test data with their labels (ds_test_label)\n",
        "- Use the encoder model to predict their latent space dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5xGEmtTRWfk"
      },
      "source": [
        "The color bar on the right indicates the intensity of\n",
        "the digit labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHhfmr29Mk85"
      },
      "outputs": [],
      "source": [
        "## Your code here\n",
        "\n",
        "images, labels =\n",
        "z_dims =\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(z_dims[:,0], z_dims[:,1], c=labels, cmap='RdYlBu', s=3)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wpJ3cteHnyk"
      },
      "source": [
        "The classes are not distributed uniformly. You can see clusters that are well separated from other classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiMrfhGKR0wy"
      },
      "source": [
        "You might be able to see the non-uniformity better in the following images, which we will\n",
        "generate by sweeping the latent variables from start of above axis to its end with a 1.0 interval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKLzcNp8So0_"
      },
      "source": [
        "- z_samples: Create a 2D numpy array of shape (100, 2) that contains [z1, z2] values in the range (-5, 5) with a 1.0 interval\n",
        "- images: Use the decoder model to construct images from those embeddings (z_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRJ339pdSWl0"
      },
      "outputs": [],
      "source": [
        "## Your code here\n",
        "z__samples=\n",
        "\n",
        "images =\n",
        "\n",
        "\n",
        "grid_col = 10\n",
        "grid_row = 10\n",
        "\n",
        "f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col, grid_row))\n",
        "\n",
        "i = 0\n",
        "for row in range(grid_row):\n",
        "    for col in range(grid_col):\n",
        "        axarr[row,col].imshow(images[i,:,:], cmap='gray')\n",
        "        axarr[row,col].axis('off')\n",
        "        i += 1\n",
        "f.tight_layout(0.1, h_pad=0.2, w_pad=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYIw40qxfxq7"
      },
      "source": [
        "This widget allows you to slide the latent variable\n",
        "bars to generate images interactively. Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKXh_USafiOY"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "@interact\n",
        "def explore_latent_variable(z1 = (-5,5,0.1),\n",
        "                            z2 = (-5,5,0.1)):\n",
        "    z_samples = [[z1, z2]]\n",
        "    images = decoder.predict(z_samples)\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(images[0,:,:], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJoaPKhcUUmD"
      },
      "source": [
        "We can see that some digits are well represented in the sample distribution and they\n",
        "are nicely drawn too. It is not the case for some other digits, which are blurry, and some\n",
        "digits are even missing from the samples. The latter shows the shortcoming where there is\n",
        "very little variation in generated images for those classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lwl6870xD2S"
      },
      "source": [
        "## 1.2 Anomaly Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ThdwgoZnGaf"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF67u25snPmD"
      },
      "source": [
        "In this example, you will train an autoencoder to detect anomalies on the ECG5000 dataset. This dataset contains 5,000 Electrocardiograms, each with 140 data points. You will use a simplified version of the dataset, where each example has been labeled either 0 (corresponding to an abnormal rhythm), or 1 (corresponding to a normal rhythm). You are interested in identifying the abnormal rhythms.\n",
        "\n",
        "Note: This is a labeled dataset, so you could phrase this as a supervised learning problem. The goal of this example is to illustrate **anomaly detection** concepts you can apply to larger datasets, where you do not have labels available (for example, if you had many thousands of normal rhythms, and only a small number of abnormal rhythms).\n",
        "\n",
        "How will you detect anomalies using an autoencoder? Recall that an autoencoder is trained to minimize reconstruction error. *You will train an autoencoder on the normal rhythms only, then use it to reconstruct all the data. Our hypothesis is that the abnormal rhythms will have higher reconstruction error. You will then classify a rhythm as an anomaly if the reconstruction error surpasses a fixed threshold.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQMuX4e1nhbM"
      },
      "source": [
        "The dataset you will use is based on one from [timeseriesclassification.com](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6YtexjLnPIX"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
        "raw_data = dataframe.values\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-D9026n04f"
      },
      "source": [
        "The last column contains the labels. The other data points are the electrocadriogram data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X05dZWfNn7XR"
      },
      "source": [
        "- Read the labels and ECG data into different variables\n",
        "- Use sklearn train_test_split to split the data into **20%** test and **80%** train data\n",
        "- Use random state = 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sPFNa0vnvM7"
      },
      "outputs": [],
      "source": [
        "#### Your Code Here #####\n",
        "labels = ######\n",
        "data = #####\n",
        "train_data, test_data, train_labels, test_labels = #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pw1GrcnosJH"
      },
      "source": [
        "Normalize the data to [0,1] using min max normaliztion\n",
        "- you might need to cast data to tf.float32\n",
        "- avoid data leak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeiMIySKo0UD"
      },
      "outputs": [],
      "source": [
        "######## Your Code Here #######\n",
        "min_val =\n",
        "max_val =\n",
        "\n",
        "train_data =\n",
        "test_data ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rDgccj5pQ7q"
      },
      "source": [
        "You will train the autoencoder using only the normal rhythms, which are labeled in this dataset as 1.\n",
        "- Separate the normal rhythms data row from the abnormal rhythms into different variables.\n",
        "- Do this for both test and train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwvCPsCkpUat"
      },
      "outputs": [],
      "source": [
        "######### Your Code Here ##########\n",
        "train_labels = train_labels.astype(bool)\n",
        "test_labels = test_labels.astype(bool)\n",
        "\n",
        "normal_train_data =\n",
        "normal_test_data =\n",
        "\n",
        "anomalous_train_data =\n",
        "anomalous_test_data ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9GORMbHp-rl"
      },
      "source": [
        "Plot a normal ECG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0zN1TV1p8-1"
      },
      "outputs": [],
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(140), normal_train_data[0])\n",
        "plt.title(\"A Normal ECG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lxk8NKMqDFK"
      },
      "source": [
        "Plot an anomalous ECG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B6P0OBNqFSS"
      },
      "outputs": [],
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(140), anomalous_train_data[0])\n",
        "plt.title(\"An Anomalous ECG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VCAVbFQqMlb"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0itn7QqqOhy"
      },
      "source": [
        "Build **TWO** fully connected models:\n",
        "1. Encoder:\n",
        "  - layer 1 : 32 neurons\n",
        "  - layer 2 : 16 neurons\n",
        "  - layer 3 : 8 neurons\n",
        "2. Decoder:\n",
        "  - layer 1: 16 neurons\n",
        "  - layer 2: 32 neurons\n",
        "  - layer 3: 140 neurons\n",
        "\n",
        "- All layers use ReLu activation except last layer should have a sigmoid avtivation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXe7RR9GqOGI"
      },
      "outputs": [],
      "source": [
        "#### Your Code Here\n",
        "encoder=\n",
        "decoder=\n",
        "autoencoder="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X1ekL8zrlsl"
      },
      "source": [
        "Compile the model using adam optimizer and mean absolute error loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji3yMkv7rdXg"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzTeb3zVrvmh"
      },
      "source": [
        "- Train the autoencoder using only the normal ECGs data\n",
        "- Use entire test data for validation\n",
        "- Use a batch size = 512\n",
        "- epochs = 20\n",
        "- shuffe the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mshDfqi9r-IN"
      },
      "outputs": [],
      "source": [
        "#### Your Code Here\n",
        "history="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvxhaAU4sNdA"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvGkf4UDszqg"
      },
      "source": [
        "### Detect Anamolies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgYlfkZhsQ2J"
      },
      "source": [
        "You will classify an ECG as anomalous if the reconstruction error is greater than **one standard deviation** from the normal training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSmvY6lqsVhb"
      },
      "source": [
        "First, let's plot a normal ECG from the training set, the reconstruction after it's encoded and decoded by the autoencoder, and the reconstruction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYpV5j0zsXZl"
      },
      "outputs": [],
      "source": [
        "encoded_data = encoder(normal_test_data).numpy()\n",
        "decoded_data = decoder(encoded_data).numpy()\n",
        "\n",
        "plt.plot(normal_test_data[0], 'b')\n",
        "plt.plot(decoded_data[0], 'r')\n",
        "plt.fill_between(np.arange(140), decoded_data[0], normal_test_data[0], color='lightcoral')\n",
        "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py7zxoPjsryo"
      },
      "source": [
        "Create a similar plot, this time for an anomalous test example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIyBkeqass07"
      },
      "outputs": [],
      "source": [
        "##### Your Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2dSoi37s532"
      },
      "source": [
        "Calculate the **mean average error** for normal examples from the training set, then classify future examples as anomalous if the *reconstruction error is higher than one standard deviation from the training set*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcFwxPqbtiW8"
      },
      "source": [
        "- Predict the ECG reconstructions for **normal ECG train data** only\n",
        "- Calculate the mean absolute error between the predicted reconstructions and the true ECG data (use tf.keras.losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnEwJG8ntG9n"
      },
      "outputs": [],
      "source": [
        "##### Your Code Here\n",
        "reconstructions =\n",
        "train_loss =\n",
        "\n",
        "\n",
        "\n",
        "plt.hist(train_loss[None,:], bins=50)\n",
        "plt.xlabel(\"Train loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSiO1XLatXVw"
      },
      "source": [
        "- Choose a threshold value that is one standard deviations above the mean of the train_loss you just calculated\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPET5Vi7tXE6"
      },
      "outputs": [],
      "source": [
        "##### Your Code Here\n",
        "mean =\n",
        "std =\n",
        "threshold=\n",
        "\n",
        "print(\"Threshold: \", threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-GtcXVwtDUD"
      },
      "source": [
        "Note: There are other strategies you could use to select a threshold value above which test examples should be classified as anomalous, the correct approach will depend on your dataset. You can learn more with the links at the end of this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuAjwwMGuhAh"
      },
      "source": [
        "If you examine the reconstruction error for the anomalous examples in the test set, you'll notice most have greater reconstruction error than the threshold. By varing the threshold, you can adjust the [precision](https://developers.google.com/machine-learning/glossary#precision) and [recall](https://developers.google.com/machine-learning/glossary#recall) of your classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxTdefOiuj-l"
      },
      "source": [
        "- Predict the ECG reconstructions for **abnormal ECG test data** only\n",
        "- Calculate the mean absolute error between the predicted reconstructions and the true ECG data (use tf.keras.losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5C8BzvjuuU1"
      },
      "outputs": [],
      "source": [
        "### Your Code Here\n",
        "reconstructions =\n",
        "test_loss =\n",
        "\n",
        "\n",
        "plt.hist(test_loss[None, :], bins=50)\n",
        "plt.xlabel(\"Test loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vvhwjNu7mP"
      },
      "source": [
        "Classify an ECG as an anomaly if the reconstruction error is greater than the threshold.\n",
        "- Fill in the missing code for the predict function to do so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5P78UybvFir"
      },
      "outputs": [],
      "source": [
        "def predict(model, data, threshold):\n",
        "  '''\n",
        "  input: the model\n",
        "  data: test data (contains raw data)\n",
        "  threshold for anomaly classification\n",
        "  '''\n",
        "  ## Use model to predict reconstructions for data\n",
        "  reconstructions =\n",
        "  ## Calculate mae loss between reconstructions and true data\n",
        "  loss =\n",
        "  ## not_nomaly should contain 1 if loss < threshold and 0 otherwise\n",
        "  not_anomaly =\n",
        "\n",
        "\n",
        "  return not_anomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhXvmSlyv-nK"
      },
      "outputs": [],
      "source": [
        "def print_stats(predictions, labels):\n",
        "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
        "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
        "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ODTi0yNwXc6"
      },
      "source": [
        "Run the following cell to look at the classification acccuracy of your anomaly detection model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdU3wZ57wU3n"
      },
      "outputs": [],
      "source": [
        "preds = predict(autoencoder, test_data, threshold)\n",
        "print_stats(preds, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LiiNQoFwx6C"
      },
      "source": [
        "# Extra Sources\n",
        "- Anomaly Detection\n",
        "  - To learn more about anomaly detection with autoencoders, look at [interactive example](https://anomagram.fastforwardlabs.com/#/) built with TensorFlow.js by Victor Dibia.\n",
        "  - For a real-world use case, you can learn how [Airbus Detects Anomalies in ISS Telemetry Data](https://blog.tensorflow.org/2020/04/how-airbus-detects-anomalies-iss-telemetry-data-tfx.html) using TensorFlow."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}